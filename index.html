
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta http-equiv="Content-Language" content="en-us">
<title>ECE 5760: Final Project</title>
<link rel="stylesheet" type="text/css" media="all" href="./format/cornell.css">
<link rel="stylesheet" type="text/css" media="all" href="./format/cornell2.css">
<link rel="stylesheet" type="text/css" media="all" href="./format/main.css">
<meta name="author" content="Jay Fetter, Tian Yao, Raghava Kumar">
<meta name="copyright" content="Copyright (c) 2017 Jay Fetter, Tian Yao, Raghava Kumar">
<meta name="description" content="ROS Enabled Stereo Vision Accelerator">
<meta name="keywords" content="microcontroller, ECE, 5760, Cornell"></head>
<body>

<div id="header">
  <!-- The following div contains the Cornell University logo and search link -->
  <div id="cu-identity"> 
		<div id="cu-logo"> 
			<a href="http://www.ece.cornell.edu/"><img src="./pics/cu_logo.gif" alt="Cornell University" width="340" height="75" border="0"></a> 
		</div> 
  </div>
  
  <div class="linklist"> <a name="top"></a> </div>
  <!-- The search-form div contains a form that allows the user to search 
		either pages or people within cornell.edu directly from the banner.	-->
  <div id="search-form">
    <form action="http://www.cornell.edu/search/" method="get" enctype="application/x-www-form-urlencoded">
      <div id="search-input">
        <label for="search-form-query">SEARCH:</label>
        <input type="text" id="search-form-query" name="q" value="" size="20">
        <input type="submit" id="search-form-submit" name="submit" value="go">
      </div>
      <div id="search-filters">
        <input type="radio" id="search-filters1" name="tab" value="" checked="checked">
        <label for="search-filters1">Pages</label>
        <input type="radio" id="search-filters2" name="tab" value="people">
        <label for="search-filters2">People</label>
        <a href="http://www.cornell.edu/search/">more options</a>
      </div>
    </form>
  </div>
</div>

<div id="mainnav">
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#design">Design</a></li>
    <li><a href="#results">Results</a></li>
    <li><a href="#conclusions">Conclusions</a></li>
    <li><a href="#appendices">Appendices</a></li>
  </ul>
</div>

<div id="sectiontitle">
  <h4><a href="http://people.ece.cornell.edu/land/courses/ece5760/">ECE 5760</a>: <a href="http://people.ece.cornell.edu/land/courses/ece5760/FinalProjects/">Final Project</a></h4>
  <h1>ROS Stereo Vision Accelerator</h1>
  <h2></h2>
  <h3>Jay Fetter(<a href="mailto:jdf258@cornell.edu">jdf258@cornell.edu</a>)</h3>
  <h3>Tian Yao (<a href="mailto:ty252@cornell.edu<">ty252@cornell.edu</a>)</h3>
  <h3>Raghava Kumar (<a href="mailto:rk534@cornell.edu<">rk534@cornell.edu</a>)</h3>
</div>

<div id="wrapper">
<div id="content">
<div id="maincontent" class="hub">

  <br><br>
  <div id="intro">
    <h2>Introduction &nbsp;&nbsp;&nbsp;<font size="-1"><strong><a href="#top">top</a></strong></font></h2>
    <p>
    A stereo camera is a camera that, through the use of two or more lenses, can capture three-dimensional images. 
	Our project turns the FPGA into a device that is able to reduce the time it takes to generate stereo images from rectified image sources. 
	We then integrated our system into a ROS node, which allows our hardware to be easily used by any ROS system. 
	</p>
	<p>
	Our node subscribes to rectified images and publishes stereo images. 
	We process the input images by passing them through a convolution filter followed by a sparse census transform. 
	We then pass the output of the census transform to a census windowing module. This procedure preprocesses the image which is then passed into the correspondence module to finally generate the stereo images. 
	</p>
	<p>
	We were successful in our project and implementing the stereo image computation algorithms. 
	We were able to accelerate this process from a 1 minute and 30 second generation time to a few milliseconds on our hardware.

    </p>
  </div>

  <div class="linklist"> <a name="overview"></a>
    <h2>High Level Design &nbsp;&nbsp;&nbsp;<font size="-1"><strong><a href="#top">top</a></strong></font></h2>
    <h3>Rationale and Inspiration:</h3>
    <p>
	Stereo Cameras are expensive and an integral part of modern day robotics systems.
	Since processing these images for generating disparity maps is computationally intensive, we felt that by using an FPGA to implement the image processing algorithms, 
	we could achieve a much higher processing rate than conventional systems. 
    </p>
	<p>
The inspiration for this project comes from our group’s real world experiences. 
Our group has experience working with stereo and other camera technologies through research and internships and the high cost of these systems inspired us to make a computationally and cost efficient stereo image accelerator. 
Since ROS is one of the most popular modelling software for robotics and automation, it made sense for us to extend our project into a ROS node.
     </p>
    <h3>Background Math</h3>
   <p>The stereo algorithm works by combining the filtering and processing algorithms listed below into a single pipeline.<br/></p>
<h4>Convolutional Filter -</h4> <br/>
<p>The input images first go through a convolutional filter. 
While our convolution module can use any kernel of any size, we chose to implement a 3x3 Gaussian blur filter due to its simplicity and good results.
 This helped to reduce any random noise in the images while preserving the edge structures which is needed for a reliable census transform (discussed in ii). 
 We also tested an averaging filter and bilateral filter in MATLAB. In comparison, the simple averaging filter gave worse results due to poor noise filtering and the bilateral filter gave slightly better results due to its ability to preserve sharper edges. The time we would have to spend on developing the bilateral filter did not justify its minor benefits for our project, so we chose to use the Gaussian blur filter. In our Verilog design, the convolutional filter expects an 8-bit unsigned number for the image pixel input and a 4.4 signed fixed point decimal for the kernel input (this gave a range of -8 to 7.9375 for the kernel values with a precision of 0.0625). The output of the filter is strictly an 8-bit unsigned value. If the convolution at a certain location produces a negative value, that value is clipped to 0. 
The Gaussian blur filter uses the following kernel weights:</p>
<!--[convolvekernel.svg]-->
<h4>Sparse Census Transform -</h4> 
	<p>
	After the images are prefiltered, they are processed by a sparse census transform. 
	The census transform works by comparing a pixel’s surrounding neighbor pixels to itself in intensity. 
	If a neighbor pixel is less in intensity, then a 1 is outputted, otherwise, a 0 is outputted. 
	The sparse census transform works by using a larger window size so that the comparisons are done on further away pixels instead of immediately adjacent pixels.
	This boosts the robustness of the algorithm to noise. The algorithm is proposed by the paper here [http://ieeexplore.ieee.org/document/6213095/].
	 The following diagram represents the flow of the census transform.
	 The transform is performed on a 5x5 window of pixels and the output is represented as a bit vector. 
	 The * represents pixels that we do not care about because we only perform the comparisons on the outer pixels. The center pixel is discarded because the output is always 0. 
 </p>
<!--[census.svg]-->
<h4>Census Windowing - </h4>
<p>As a method to improve the output quality and also be more robust to noise, we perform a 3x3 windowing the on the census output. 
The module simply concatenates the nine 8-bit vectors inside the window into a single 72-bit vector. The following diagram shows the process on a 2x2 window. </p>
<!--[censuswnd.svg]-->
<h4>Correlation - </h4>
<p>The correlation step take the windows census bit vectors from the left and right images and compares them to generate a disparity map. 
Our design can generate a depth map with 64 levels, so for each census vector in the right image, we compare it with 64 census vectors in the left image to find the level with the lowest difference between the two bit vectors. 
For each comparison, we XOR the two bit vectors and sum up the bits of the output vector (this is formally called the Hamming distance). 
If the XOR output for a certain bit is 1, then that means the bits are different between the left and right image, therefore the sum represents the amount of disparity between the two bit vectors. 
We then perform a tree comparison of the 64 disparity values to find the lowest disparity value. The index of of that value is the corresponding depth of that pixel in the original image. 
An index of 0 means that the object is infinitely far away and an index of 1 means that the object is very close. The following diagram shows the correlation process:</p>
<!--[correlate.svg]-->

        
  <div class="linklist" title=""> <a name="design"></a>
    <h2>Design &nbsp;&nbsp;&nbsp;<font size="-1"><strong><a href="#top">top</a></strong></font></h2>
    <!-- PUT THINGS TRIED BUT DID NOT WORK -->
    <h5>Hardware Design</h5>

   
    <h5>Software Design</h5>
   
  <h5>Performance:</h5>
  
  <div class="linklist" title=""> <a name="conclusions"></a>
    <h2>Conclusions &nbsp;&nbsp;&nbsp;<font size="-1"><strong><a href="#top">top</a></strong></font></h2>
   
    
    
    
    <h4>Ethical Considerations</h4>
   
    <h4>Legal Considerations</h4>
  
    
  </div> <!-- conclusion -->

  <div class="linklist" title=""> <a name="appendices"></a>
    <h2>Appendices &nbsp;&nbsp;&nbsp;<font size="-1"><strong><a href="#top">top</a></strong></font></h2>
    <h3>A. Code Listing</h3>
    
  
    <h3>B. Distribution of work:</h3>
     
      
      
    <div class="linklist"> <a name="references"></a>
     
    </div>
             
    <div class="linklist"> <a name="ack"></a>
      <h2>Acknowledgements &nbsp;&nbsp;&nbsp;<font size="-1"><strong><a href="#top">top</a></strong></font></h2>
    </div>
  </div> <!-- appendix -->
    
  <hr>
  <div id="footerwrap">
    <div id="footer">
      <div id="copyright">
        <div class="copyright">©2016 Jay Fetter, Tian Yao, Raghava Kumar</div>
        <div class="copyright">Layout ©2010 Cornell University</div>
      </div>
    </div>
  </div>
</div>
